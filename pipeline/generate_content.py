# pipeline/generate_content.py
import os
import sys
import json
import time
import csv
from pathlib import Path
from dotenv import load_dotenv
import requests
from pipeline.image_generator import generate_image
from pipeline.generate_prompts_via_groq_auto import make_image_prompt

# schema validator (created separately)
from pipeline.utils.schema_validator import validate_json


import subprocess
# already have: import os, sys, json, time, ...
# ensure ROOT is available (you already define ROOT earlier)
REQUIRED_SOCIAL_COLUMNS = ["platform", "date", "time(IST)", "caption", "image_prompt", "hashtags", "utm", "post_type"]


# load .env so GROQ_API_KEY etc. are available
load_dotenv()

ROOT = Path(__file__).parent.parent
OUT = ROOT / "outputs"
PROMPTS_DIR = OUT / "prompts"
LLM_RESULTS_DIR = OUT / "llm_results"
BLOG_OUT_DIR = OUT / "blog"
MDX_OUT_DIR = OUT / "mdx"
SOCIAL_OUT_DIR = OUT / "social"
ADS_OUT_DIR = OUT / "ads"
SEO_OUT_DIR = OUT / "seo"
LOGS_DIR = OUT / "logs"

for d in [PROMPTS_DIR, LLM_RESULTS_DIR, BLOG_OUT_DIR, MDX_OUT_DIR, SOCIAL_OUT_DIR, ADS_OUT_DIR, SEO_OUT_DIR, LOGS_DIR]:
    d.mkdir(parents=True, exist_ok=True)

# Mode (manual/api) controlled by env var CONTENT_MODE
MODE = os.getenv("CONTENT_MODE", "manual").lower()
PROVIDER = os.getenv("CONTENT_PROVIDER", "").upper()

# --- Helper: load context.json ----------------
def load_context():
    p = OUT / "context.json"
    if not p.exists():
        raise FileNotFoundError("context.json not found. Run scrapers and process_data first.")
    return json.load(open(p, encoding="utf-8"))

# --- Prompt builders --------------------------
def build_blog_prompt(context):
    trends = context.get("trends", {}).get("related", {})
    sample_comp = (context.get("competitors") or [])[:2]
    comp_snippets = "\n\n".join([f"- {c.get('title','')}: {c.get('snippet','')[:400]}" for c in sample_comp])

    prompt = f"""You are a professional content writer for Calyco (online-first, performance-focused paint brand).
Context (trends + competitors):
{json.dumps(trends, indent=2)[:3000]}

Competitor snippets (examples):
{comp_snippets}

Requirements:
- Title: include season/year (e.g., "Trending Paint Colors for 2026")
- Tone: helpful, modern, performance-first (mention low-VOC, washable, fast recoat)
- Structure: introduction, at least 4 subheads, bullets where helpful, conclusion
- Include: meta_title (<=60 chars), meta_description (<=155 chars), 3 FAQs, and 3 image prompt ideas (hero + 2 supporting)
- Do NOT mention AI or say content was generated by AI.
Return: clear sections for meta and article body.
"""
    return prompt.strip()

def build_mdx_prompt(context):
    prompt = """Create MDX frontmatter + content for a product page for "Calyco Super Emulsion - Interior".
Include:
- Frontmatter (title, slug, meta.title, meta.description)
- Short descriptive paragraph (2-4 sentences)
- 3 feature bullets (highlight low-VOC, washable, fast recoat)
- Technical specs table (coverage per litre, drying time, recommended coats)
- CTA (1 short sentence)
Output: MDX (YAML frontmatter + markdown body).
Do NOT mention AI."""
    return prompt

def build_social_prompt(context):
    trends_keys = list((context.get("trends", {}) or {}).get("related", {}).keys())[:6]
    prompt = f"""Given these trending keywords: {trends_keys}
Generate:
- 3 Instagram consumer posts (caption, suggested hashtags, image prompt)
- 3 LinkedIn contractor posts (caption, suggested hashtags, image prompt)
Format: CSV rows with columns:
platform,date,time(IST),caption,image_prompt,hashtags,utm,post_type
Rules: concise, on-brand (low-VOC, washable), never mention AI.
"""
    return prompt

def build_ad_prompt(context):
    return """Generate 6 short ad snippets (Google/FB/WhatsApp). For each: channel, headline (<=30 chars), description (<=90 chars), CTA. Output as CSV rows."""

# --- Save prompts (manual mode) ----------
def save_prompt(name, content):
    path = PROMPTS_DIR / f"{name}.txt"
    path.write_text(content, encoding="utf-8")
    print("Wrote prompt:", path)
    return path

# --- GROQ API send function ----------------
def send_to_api(prompt: str, task: str = "blog", model: str = None) -> str:
    API_KEY = os.getenv("GROQ_API_KEY")
    if not API_KEY:
        raise RuntimeError("GROQ_API_KEY not set. Add it to .env or env vars.")
    if model is None:
        model = "llama-3.1-8b-instant"
    url = "https://api.groq.com/openai/v1/chat/completions"
    headers = {
        "Authorization": f"Bearer {API_KEY}",
        "Content-Type": "application/json"
    }
    max_tokens = 3000 if task == "blog" else 1500
    payload = {
        "model": model,
        "messages": [
            {"role": "system", "content": "You are a professional content writer for Calyco. Never say 'AI' or that content is generated by AI. Keep brand voice consistent."},
            {"role": "user", "content": prompt}
        ],
        "max_tokens": max_tokens,
        "temperature": 0.2
    }
    resp = requests.post(url, headers=headers, json=payload, timeout=300)
    if resp.status_code != 200:
        # save error in logs and raise
        err_path = LOGS_DIR / f"groq_error_{int(time.time())}.json"
        err_path.write_text(resp.text, encoding="utf-8")
        raise RuntimeError(f"GROQ API error {resp.status_code}: saved to {err_path}")
    data = resp.json()
    # parse common shapes
    if "choices" in data and isinstance(data["choices"], list) and data["choices"]:
        first = data["choices"][0]
        msg = first.get("message") or first.get("delta") or {}
        content = None
        if isinstance(msg, dict):
            content = msg.get("content")
            if isinstance(content, list):
                parts = []
                for c in content:
                    if isinstance(c, dict):
                        parts.append(c.get("text") or c.get("content") or "")
                    elif isinstance(c, str):
                        parts.append(c)
                content = "".join(parts)
        if not content:
            content = first.get("text") or ""
        return content or json.dumps(data, ensure_ascii=False)
    if "output_text" in data:
        return data["output_text"]
    return json.dumps(data, ensure_ascii=False)[:20000]

# --- Manual mode: write prompts and placeholders ------------
def save_manual_prompts():
    ctx = load_context()
    prompts = {
        "blog_prompt": build_blog_prompt(ctx),
        "mdx_prompt": build_mdx_prompt(ctx),
        "social_prompt": build_social_prompt(ctx),
        "ad_prompt": build_ad_prompt(ctx)
    }
    for name, p in prompts.items():
        save_prompt(name, p)
        stub = LLM_RESULTS_DIR / f"{name}_response.txt"
        if not stub.exists():
            stub.write_text(f"# Paste the LLM output for {name} here\n", encoding="utf-8")
    print("Manual prompts saved to outputs/prompts/. Paste responses into outputs/llm_results/ and run with 'postprocess'.")

# --- utility ---
def _safe_text_summary(raw, maxchars=150):
    s = raw.strip()
    s = " ".join(s.splitlines())[:maxchars]
    return s

# --- Postprocess manual pasted outputs into artifacts ------------
def postprocess_llm_results():
    # BLOG
    blog_in = LLM_RESULTS_DIR / "blog_prompt_response.txt"
    if blog_in.exists():
        raw = blog_in.read_text(encoding="utf-8").strip()

        # extract title and meta if present, else make sensible defaults
        lines = [l.strip() for l in raw.splitlines() if l.strip()]
        title = lines[0][:200] if lines else f"Blog {int(time.time())}"
        meta_title = title[:60]
        meta_description = _safe_text_summary(raw, maxchars=155)

        # Convert body into content blocks expected by schema (array)
        content_blocks = [
            {"type": "paragraph", "text": p.strip()}
            for p in raw.split("\n\n")
            if p.strip()
        ]
        if not content_blocks:
            content_blocks = [{"type": "paragraph", "text": raw[:300]}]

        blog_obj = {
            "title": title,
            "meta": {
                "title": meta_title,
                "description": meta_description
            },
            "content": content_blocks,
            "datePublished": time.strftime("%Y-%m-%d"),
            "generated_at": time.strftime("%Y-%m-%d %H:%M:%S"),
            # backward-compatible legacy fields
            "meta_title": meta_title,
            "meta_description": meta_description,
            "body": raw
        }

        out_path = BLOG_OUT_DIR / f"blog_{int(time.time())}.json"
        out_path.write_text(json.dumps(blog_obj, indent=2, ensure_ascii=False), encoding="utf-8")
        print("Saved blog JSON:", out_path)

        # === Validate blog JSON against schema ===
        try:
            schema_path = os.path.join("pipeline", "schemas", "blog_schema.json")
            ok, err = validate_json(blog_obj, schema_path)
            if not ok:
                print("❌ Blog schema validation failed:", err)
            else:
                print("✅ Blog schema validated successfully.")
        except Exception as e:
            print("Error while validating blog JSON:", e)

        # === Generate images for this blog (hero + 2 supporting) ===
        # We will create 3 image prompts from the blog metadata using make_image_prompt(),
        # then call generate_image() which will either call the OpenAI API (if OPENAI_API_KEY
        # is present) or create a black placeholder (if not).
        try:
            content_meta = {
                "title": blog_obj.get("title"),
                # try to derive a color_family or tags from the content if available;
                # fallback to neutral if none present
                "color_family": blog_obj.get("meta", {}).get("color_family", "") or "",
                "trend_keywords": [],
                "size_hint": "1024x1792"  # hero default; make_image_prompt can override if you set different
            }

            # Hero image
            hero_prompt, hero_size = make_image_prompt(content_meta, image_type="hero")
            hero_path = generate_image(hero_prompt, size=hero_size)

            # Support image 1 (product / texture close-up)
            support1_meta = dict(content_meta)
            support1_meta["size_hint"] = "1024x1024"
            support1_prompt, support1_size = make_image_prompt(support1_meta, image_type="product")
            support1_path = generate_image(support1_prompt, size=support1_size)

            # Support image 2 (social / lifestyle)
            support2_meta = dict(content_meta)
            support2_meta["size_hint"] = "1024x1024"
            support2_prompt, support2_size = make_image_prompt(support2_meta, image_type="social")
            support2_path = generate_image(support2_prompt, size=support2_size)

            # Attach generated image metadata to the blog JSON so front-end can use it
            blog_obj["image_path_hero"] = hero_path
            blog_obj["image_prompt_hero"] = hero_prompt
            blog_obj["image_path_support1"] = support1_path
            blog_obj["image_prompt_support1"] = support1_prompt
            blog_obj["image_path_support2"] = support2_path
            blog_obj["image_prompt_support2"] = support2_prompt

            # rewrite the blog json to include image fields
            out_path.write_text(json.dumps(blog_obj, indent=2, ensure_ascii=False), encoding="utf-8")
            print("Updated blog JSON with image paths:", out_path)

        except Exception as e:
            print("Image generation step failed for blog:", e)

    # MDX
    mdx_in = LLM_RESULTS_DIR / "mdx_prompt_response.txt"
    if mdx_in.exists():
        raw = mdx_in.read_text(encoding="utf-8").strip()
        out_path = MDX_OUT_DIR / f"product_super_emulsion_{int(time.time())}.mdx"
        out_path.write_text(raw, encoding="utf-8")
        print("Saved MDX:", out_path)

    # SOCIAL
    social_in = LLM_RESULTS_DIR / "social_prompt_response.txt"
    if social_in.exists():
        raw = social_in.read_text(encoding="utf-8").strip()
        rows = []
        for line in raw.splitlines():
            if line.strip() and "," in line:
                rows.append(line.strip())
        if rows:
            csv_path = SOCIAL_OUT_DIR / "social_posts.csv"
            with open(csv_path, "w", encoding="utf-8", newline="") as f:
                for r in rows:
                    f.write(r + "\n")
            print("Saved social CSV:", csv_path)
        else:
            out_path = SOCIAL_OUT_DIR / f"social_raw_{int(time.time())}.txt"
            out_path.write_text(raw, encoding="utf-8")
            print("Saved social raw output:", out_path)

    # ADS
    ads_in = LLM_RESULTS_DIR / "ad_prompt_response.txt"
    if ads_in.exists():
        raw = ads_in.read_text(encoding="utf-8").strip()
        out_path = ADS_OUT_DIR / f"ads_{int(time.time())}.csv"
        out_path.write_text(raw, encoding="utf-8")
        print("Saved ads:", out_path)

    # SEO JSON-LD from latest blog
    try:
        latest_blog = None
        for f in sorted(BLOG_OUT_DIR.iterdir(), reverse=True):
            if f.suffix.lower() == ".json":
                latest_blog = f
                break
        if latest_blog:
            j = json.load(open(latest_blog, encoding="utf-8"))
            jd = {
                "@context": "https://schema.org",
                "@type": "Article",
                "headline": j.get("meta", {}).get("title") or j.get("meta_title"),
                "description": j.get("meta", {}).get("description") or j.get("meta_description"),
                "datePublished": j.get("datePublished") or j.get("generated_at"),
                "author": {"@type": "Organization", "name": "Calyco Paints"},
            }
            out = SEO_OUT_DIR / f"{latest_blog.stem}.jsonld"
            out.write_text(json.dumps(jd, indent=2, ensure_ascii=False), encoding="utf-8")
            print("Saved JSON-LD:", out)

            # === Validate SEO JSON-LD ===
            try:
                seo_schema = os.path.join("pipeline", "schemas", "seo_schema.json")
                ok, err = validate_json(jd, seo_schema)
                if not ok:
                    print("❌ SEO JSON-LD schema failed:", err)
                else:
                    print("✅ SEO JSON-LD schema passed.")
            except Exception as e:
                print("Error while validating SEO JSON-LD:", e)
    except Exception as e:
        print("Could not emit JSON-LD:", e)

    print("Postprocessing complete. Run pipeline/rules.py to validate outputs.")

# --- API run: call GROQ, save raw responses, then postprocess ------------
def api_mode_run():
    ctx = load_context()
    prompts = {
        "blog": build_blog_prompt(ctx),
        "mdx": build_mdx_prompt(ctx),
        "social": build_social_prompt(ctx),
        "ad": build_ad_prompt(ctx)
    }
    for name, p in prompts.items():
        print("Calling API for:", name)
        try:
            content = send_to_api(p, task=name)
        except Exception as e:
            print("API error for", name, ":", e)
            # write error placeholder
            err_file = LLM_RESULTS_DIR / f"{name}_error.txt"
            err_file.write_text(str(e), encoding="utf-8")
            continue
        # save raw response
        out_raw = LLM_RESULTS_DIR / f"{name}_prompt_response.txt"
        out_raw.write_text(content, encoding="utf-8")
        print("Saved raw response:", out_raw)
        time.sleep(1.0)

    # once API outputs are saved, postprocess them
    postprocess_llm_results()

# --- CLI entrypoint ---------------------------
def main():
    if len(sys.argv) > 1 and sys.argv[1] == "postprocess":
        postprocess_llm_results()
        return
    if MODE == "manual":
        save_manual_prompts()
        print("Manual mode complete. Paste LLM outputs and run: python pipeline/generate_content.py postprocess")
    elif MODE == "api":
        print("Running in API mode with provider:", PROVIDER)
        api_mode_run()
        print("API mode complete. Run pipeline/rules.py to validate.")
    else:
        print("Unknown MODE:", MODE)
        save_manual_prompts()

if __name__ == "__main__":
    main()
